# Minimal dependencies for on-device memory orchestration
# No FastAPI/uvicorn needed - using direct function calls

# Core dependencies
numpy>=1.24.0          # For embedding operations
requests>=2.31.0       # For Cerebras API calls

# Apple Silicon optimized ML (optional, will use fallback if not available)
mlx>=0.14.0            # For fast embeddings on Apple Silicon

# Fallback embedding model (if MLX not available)
sentence-transformers>=2.2.0  # For fallback embeddings
torch>=2.0.0          # Required by sentence-transformers